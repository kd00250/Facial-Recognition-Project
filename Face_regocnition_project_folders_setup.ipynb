{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jYQfW59F1x2T"
      },
      "outputs": [],
      "source": [
        "!mkdir Face_regocnition_project && cd Face_regocnition_project"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Face_regocnition_project/data_preprocessing.py\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "def extract_and_arrange(zip_file=\"Faces.zip\", extracted_folder=\"Faces_unorganized\", structured_folder=\"Faces\"):\n",
        "    print(\"Extracting zip file...\")\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extracted_folder)\n",
        "    print(\"Extraction completed. Files extracted into:\", extracted_folder)\n",
        "\n",
        "    print(\"Arranging files into structured folders...\")\n",
        "    os.makedirs(structured_folder, exist_ok=True)\n",
        "\n",
        "    # Recursively walk through the extracted folder\n",
        "    for root, dirs, files in os.walk(extracted_folder):\n",
        "        for filename in files:\n",
        "            if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                # Extract label: everything before the underscore followed by digits.\n",
        "                match = re.match(r'^(.*?)_\\d+', filename)\n",
        "                if match:\n",
        "                    label = match.group(1)\n",
        "                else:\n",
        "                    label = os.path.splitext(filename)[0]\n",
        "\n",
        "                # Create a subfolder for this label if it doesn't exist.\n",
        "                label_dir = os.path.join(structured_folder, label)\n",
        "                os.makedirs(label_dir, exist_ok=True)\n",
        "\n",
        "                # Define source and destination paths.\n",
        "                src = os.path.join(root, filename)\n",
        "                dst = os.path.join(label_dir, filename)\n",
        "                shutil.move(src, dst)\n",
        "    print(\"Dataset arranged in folder:\", structured_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfXnRrUO2dkI",
        "outputId": "7222ebaf-33be-4e90-f8dc-147d543f2df6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Face_regocnition_project/data_preprocessing.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Face_regocnition_project/data_loading.py\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "def get_data_loaders(data_dir=\"Faces\", batch_size=32, image_size=(128,128)):\n",
        "    # Define transformations: resize, convert to tensor, and normalize.\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    # Load the dataset from the arranged folder\n",
        "    dataset = datasets.ImageFolder(root=data_dir, transform=data_transforms)\n",
        "    print(\"Classes found:\", dataset.classes)\n",
        "\n",
        "    # Split the dataset into training and validation sets (80-20 split)\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "    print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Return datasets, loaders and class names for later use\n",
        "    return train_dataset, val_dataset, train_loader, val_loader, dataset.classes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWEaqeth2s2_",
        "outputId": "116fa9b6-6497-40ab-cf42-a7e911c8b251"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Face_regocnition_project/data_loading.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Face_regocnition_project/model_training.py\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "class ModelTrainer:\n",
        "    def __init__(self, model, train_loader, val_loader, train_dataset, val_dataset, device, criterion, optimizer, scheduler=None):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.train_dataset = train_dataset\n",
        "        self.val_dataset = val_dataset\n",
        "        self.device = device\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "    def train(self, num_epochs=10, early_stop_patience=5):\n",
        "        best_val_loss = float('inf')\n",
        "        epochs_no_improve = 0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_start_time = time.time()\n",
        "\n",
        "            # ----- Training Phase -----\n",
        "            self.model.train()\n",
        "            running_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            train_bar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
        "            for images, labels in train_bar:\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                outputs = self.model(images)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=2.0)\n",
        "                self.optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * images.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                train_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "            epoch_loss = running_loss / len(self.train_dataset)\n",
        "            epoch_accuracy = 100 * correct / total\n",
        "            print(f\"\\nEpoch [{epoch+1}/{num_epochs}] Training Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
        "\n",
        "            # ----- Validation Phase -----\n",
        "            self.model.eval()\n",
        "            val_loss = 0.0\n",
        "            correct_val = 0\n",
        "            total_val = 0\n",
        "\n",
        "            val_bar = tqdm(self.val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\")\n",
        "            with torch.no_grad():\n",
        "                for images, labels in val_bar:\n",
        "                    images, labels = images.to(self.device), labels.to(self.device)\n",
        "                    outputs = self.model(images)\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "                    val_loss += loss.item() * images.size(0)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total_val += labels.size(0)\n",
        "                    correct_val += (predicted == labels).sum().item()\n",
        "                    val_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "            val_loss = val_loss / len(self.val_dataset)\n",
        "            val_accuracy = 100 * correct_val / total_val\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "            if self.scheduler is not None:\n",
        "                self.scheduler.step(val_loss)\n",
        "            current_lr = self.optimizer.param_groups[0]['lr']\n",
        "            print(f\"Current Learning Rate: {current_lr}\")\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                torch.save(self.model.state_dict(), 'best_model.pth')\n",
        "                print(\"Model checkpoint saved!\")\n",
        "                epochs_no_improve = 0\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve >= early_stop_patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "            epoch_duration = time.time() - epoch_start_time\n",
        "            print(f\"Epoch duration: {epoch_duration:.2f} seconds\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-L8bUTU21AX",
        "outputId": "eb43bce6-1443-4727-d458-a49fb5241400"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Face_regocnition_project/model_training.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Face_regocnition_project/inference.py\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "class InferenceEngine:\n",
        "    def __init__(self, model, device, transform=None, class_names=None):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.model.eval()\n",
        "        self.transform = transform\n",
        "        self.class_names = class_names if class_names is not None else []\n",
        "\n",
        "    def predict_image(self, image_path):\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        image = image.unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(image)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "        if self.class_names:\n",
        "            return self.class_names[predicted.item()]\n",
        "        return predicted.item()\n",
        "\n",
        "    def webcam_inference(self, capture_interval=10):\n",
        "        cap = cv2.VideoCapture(0)\n",
        "        if not cap.isOpened():\n",
        "            print(\"Error: Unable to open the webcam.\")\n",
        "            return\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                print(\"Error: Unable to capture frame.\")\n",
        "                break\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            image = Image.fromarray(frame_rgb)\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            image = image.unsqueeze(0).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(image)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "            label = self.class_names[predicted.item()] if self.class_names else str(predicted.item())\n",
        "            cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                        1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "            cv2.imshow(\"Webcam Inference\", frame)\n",
        "            if cv2.waitKey(capture_interval * 1000) & 0xFF == ord('q'):\n",
        "                break\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GWXxHeU3EGc",
        "outputId": "8826ce20-0374-4a53-85db-af77261e7101"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Face_regocnition_project/inference.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Face_regocnition_project/main.py\n",
        "# Add the project folder to the Python path if needed\n",
        "import sys\n",
        "sys.path.append(\"Face_regocnition_project\")\n",
        "\n",
        "# Import your modules\n",
        "from data_preprocessing import extract_and_arrange\n",
        "from data_loading import get_data_loaders\n",
        "from model_training import ModelTrainer\n",
        "from inference import InferenceEngine\n",
        "\n",
        "# Run the preprocessing step (if not done already)\n",
        "extract_and_arrange(zip_file=\"Faces.zip\", extracted_folder=\"Faces_unorganized\", structured_folder=\"Faces\")\n",
        "\n",
        "# Load your data\n",
        "train_dataset, val_dataset, train_loader, val_loader, class_names = get_data_loaders(data_dir=\"Faces\", batch_size=32, image_size=(128,128))\n",
        "\n",
        "# Assume you have defined your model, device, criterion, optimizer, etc.\n",
        "# For example:\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define your model here or load a pre-trained model\n",
        "# For demonstration, here we create a simple model:\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(128*128*3, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, len(class_names))\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
        "\n",
        "# Initialize your trainer and start training\n",
        "trainer = ModelTrainer(model, train_loader, val_loader, train_dataset, val_dataset, device, criterion, optimizer, scheduler)\n",
        "trainer.train(num_epochs=10, early_stop_patience=5)\n",
        "\n",
        "# (Optional) Use inference\n",
        "inference_engine = InferenceEngine(model, device, transform=transforms.Compose([\n",
        "    transforms.Resize((128,128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
        "]), class_names=class_names)\n",
        "prediction = inference_engine.predict_image(\"path/to/your/image.jpg\")\n",
        "print(\"Predicted class:\", prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R53G3g7_3cE2",
        "outputId": "32e90063-9eff-41b1-c3e8-e38a894f4317"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Face_regocnition_project/main.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r Face_regocnition_project.zip Face_regocnition_project\n",
        "from google.colab import files\n",
        "files.download(\"Face_regocnition_project.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "L5OXgHVi58Mx",
        "outputId": "fd1e6aa5-4ed1-4299-8192-5a712d85d1b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: Face_regocnition_project/ (stored 0%)\n",
            "  adding: Face_regocnition_project/model_training.py (deflated 73%)\n",
            "  adding: Face_regocnition_project/data_loading.py (deflated 60%)\n",
            "  adding: Face_regocnition_project/data_preprocessing.py (deflated 61%)\n",
            "  adding: Face_regocnition_project/Faces.zip (stored 0%)\n",
            "  adding: Face_regocnition_project/inference.py (deflated 64%)\n",
            "  adding: Face_regocnition_project/main.py (deflated 53%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_42d69322-df20-43e9-ac2f-9a1ce1ff76a1\", \"Face_regocnition_project.zip\", 13576022)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}